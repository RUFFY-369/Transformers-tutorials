{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training IRIS model ported into Hugging Face transformers\n\nIn this notebook, we are going to train IRIS RL agent from [Transformers are Sample-Efficient World Models paper](https://arxiv.org/abs/2209.00588) on Atari environments to play breakout or whatever environment id you give to `make_atari` function below. The paper presents IRIS (Imagination with auto-Regression over an Inner Speech) which is a reinforcement learning agent that trains within an imagined world model, which consists of a discrete autoencoder and an autoregressive Transformer. IRIS acquires behaviors by precisely simulating millions of trajectories.\n\nThe paper's approach frames dynamics learning as a sequence modeling problem, with an autoencoder constructing a language of image tokens and a Transformer orchestrating that language over time.\n\nThere is also a [medium blog post](https://medium.com/@cedric.vandelaer/paper-review-transformers-are-sample-efficient-world-models-d0f9144f9c09) to understand how the algorithm works.\n\nI have ported IRIS into transformers and the PR is under review. The notebook acts as a testing example for the ported model before the final merge.\n\n`Note`: The notebook will be a demo for the ported model when the [PR](https://github.com/huggingface/transformers/pull/30883) gets merged.\n\nIn HuggingFace Transformers, one can directly load any [IRIS checkpoint](https://huggingface.co/models?other=iris) for different Atari environments from the hub directly into a `IrisModel` from the PR.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://huggingface.co/datasets/ruffy369/sample-images/resolve/main/iris_model_original_arch.png\"\nalt=\"drawing\"/>\n\nImage from the paper, illustrating the model's workings","metadata":{}},{"cell_type":"markdown","source":"## Set-up environment\n\nLet's begin by installing the requirements for training IRIS from the original repository.","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/eloialonso/iris","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working/iris\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install setuptools==66 #(gym(0.21)error sol)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/openai/gym.git@9180d12e1b66e7e2a1a622614f787a6ec147ac40","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now we have to reinstall transformers in dev mode for the ported model","metadata":{}},{"cell_type":"code","source":"# Ensure clean environment\n!pip uninstall transformers -y\n!pip uninstall tokenizers -y\n\n# Clone the repository\n!git clone  -b add_iris --single-branch https://github.com/RUFFY-369/transformers.git\n%cd transformers\n\n# Initialize submodules\n!git submodule update --init --recursive\n\n# Install in dev mode\n!pip install -e \".[quality]\"\n\n# Clear cache\n!find . -type d -name \"__pycache__\" -exec rm -r {} +\n\n# Restart runtime (do this manually or via menu)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### `Restart kernel after the above cell execution`","metadata":{}},{"cell_type":"markdown","source":"## Hold onto your neuronsðŸ¤—here comes the code \n\n`Note:` The code is from the original repositroy to keep the training of the ported model as accurate as possible ","metadata":{}},{"cell_type":"code","source":"cd /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Performing some necessary imports","metadata":{}},{"cell_type":"code","source":"from __future__ import annotations\nfrom typing import Any,Dict, List, Optional, Tuple,List, Union\nimport gym\nimport numpy as np\nfrom PIL import Image\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport warnings\nimport torch\n\nfrom collections import deque\nimport math\nfrom pathlib import Path\nimport random\n\nimport psutil\nimport transformers as t\nfrom transformers import Trainer, TrainingArguments\nfrom torch.utils.data import Dataset, DataLoader\nimport sys\n\nfrom einops import rearrange\nfrom tqdm import tqdm\nimport wandb\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `make_atari` is a function that creates and configures an Atari environment with optional preprocessing steps and returns sn instance of the environment with wrappers applied.","metadata":{}},{"cell_type":"code","source":"def make_atari(id, size=64, max_episode_steps=None, noop_max=30, frame_skip=4, done_on_life_loss=False, clip_reward=False):\n    env = gym.make(id)\n    assert 'NoFrameskip' in env.spec.id or 'Frameskip' not in env.spec\n    env = ResizeObsWrapper(env, (size, size))\n    if clip_reward:\n        env = RewardClippingWrapper(env)\n    if max_episode_steps is not None:\n        env = gym.wrappers.TimeLimit(env, max_episode_steps=max_episode_steps)\n    if noop_max is not None:\n        env = NoopResetEnv(env, noop_max=noop_max)\n    env = MaxAndSkipEnv(env, skip=frame_skip)\n    if done_on_life_loss:\n        env = EpisodicLifeEnv(env)\n    return env","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `ResizeObsWrapper` is a Gym wrapper that resizes observations from an environment to a specified width and height while preserving color channels.","metadata":{}},{"cell_type":"code","source":"class ResizeObsWrapper(gym.ObservationWrapper):\n    def __init__(self, env: gym.Env, size: Tuple[int, int]) -> None:\n        gym.ObservationWrapper.__init__(self, env)\n        self.size = tuple(size)\n        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(size[0], size[1], 3), dtype=np.uint8)\n        self.unwrapped.original_obs = None\n\n    def resize(self, obs: np.ndarray):\n        img = Image.fromarray(obs)\n        img = img.resize(self.size, Image.BILINEAR)\n        return np.array(img)\n\n    def observation(self, observation: np.ndarray) -> np.ndarray:\n        self.unwrapped.original_obs = observation\n        return self.resize(observation)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `RewardClippingWrapper` is a Gym wrapper that clips rewards to -1 or 1 by applying the sign function, effectively normalizing all rewards.","metadata":{}},{"cell_type":"code","source":"class RewardClippingWrapper(gym.RewardWrapper):\n    def reward(self, reward):\n        return np.sign(reward)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `NoopResetEnv` is a Gym wrapper that performs a random number of no-op actions (action 0) during environment reset to sample initial states, with the number of no-ops sampled from a range of 1 to `noop_max`.","metadata":{}},{"cell_type":"code","source":"class NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        \"\"\"Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n\n    def reset(self, **kwargs):\n        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n            if done:\n                obs = self.env.reset(**kwargs)\n        return obs\n\n    def step(self, action):\n        return self.env.step(action)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `EpisodicLifeEnv` is a Gym wrapper that treats loss of life as the end of an episode, while only resetting the environment when true game over occurs, facilitating more stable value estimation in reinforcement learning.","metadata":{}},{"cell_type":"code","source":"class EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so it's important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        \"\"\"Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        \"\"\"\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `MaxAndSkipEnv` is a Gym wrapper that performs max pooling over the last two observations and skips frames by only returning every `skip`-th frame, while accumulating rewards over skipped frames.","metadata":{}},{"cell_type":"code","source":"class MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        gym.Wrapper.__init__(self, env)\n        assert skip > 0\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n        self._skip = skip\n        self.max_frame = np.zeros(env.observation_space.shape, dtype=np.uint8)\n\n    def step(self, action):\n        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2:\n                self._obs_buffer[0] = obs\n            if i == self._skip - 1:\n                self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn't matter\n        self.max_frame = self._obs_buffer.max(axis=0)\n\n        return self.max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `DoneTrackerEnv` is a class that tracks the done status of multiple environments, monitoring if they are currently done, newly done, or already done, with methods to update and query the status.\n","metadata":{}},{"cell_type":"code","source":"class DoneTrackerEnv:\n    def __init__(self, num_envs: int) -> None:\n        \"\"\"Monitor env dones: 0 when not done, 1 when done, 2 when already done.\"\"\"\n        self.num_envs = num_envs\n        self.done_tracker = None\n        self.reset_done_tracker()\n\n    def reset_done_tracker(self) -> None:\n        self.done_tracker = np.zeros(self.num_envs, dtype=np.uint8)\n\n    def update_done_tracker(self, done: np.ndarray) -> None:\n        self.done_tracker = np.clip(2 * self.done_tracker + done, 0, 2)\n\n    @property\n    def num_envs_done(self) -> int:\n        return (self.done_tracker > 0).sum()\n\n    @property\n    def mask_dones(self) -> np.ndarray:\n        return np.logical_not(self.done_tracker)\n\n    @property\n    def mask_new_dones(self) -> np.ndarray:\n        return np.logical_not(self.done_tracker[self.done_tracker <= 1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `SingleProcessEnv` is a class for managing a single environment process that tracks done status, performs actions, and handles environment interactions including resetting, stepping, rendering, and closing.","metadata":{}},{"cell_type":"code","source":"class SingleProcessEnv(DoneTrackerEnv):\n    def __init__(self, env_fn):\n        super().__init__(num_envs=1)\n        self.env = env_fn\n        self.num_actions = self.env.action_space.n\n\n    def should_reset(self) -> bool:\n        return self.num_envs_done == 1\n\n    def reset(self) -> np.ndarray:\n        self.reset_done_tracker()\n        obs = self.env.reset()\n        return obs[None, ...]\n\n    def step(self, action) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Any]:\n        obs, reward, done, _ = self.env.step(action[0])  # action is supposed to be ndarray (1,)\n        done = np.array([done])\n        self.update_done_tracker(done)\n        return obs[None, ...], np.array([reward]), done, None\n\n    def render(self) -> None:\n        self.env.render()\n\n    def close(self) -> None:\n        self.env.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `EpisodeMetrics` is a dataclass that stores metrics for a single episode, including its length and total return.","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass EpisodeMetrics:\n    episode_length: int\n    episode_return: float","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `Episode` is a dataclass that represents an episode with observations, actions, rewards, end markers, and padding masks, providing methods for length management, merging, segmenting, computing metrics, and saving to disk.","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass Episode:\n    observations: torch.ByteTensor\n    actions: torch.LongTensor\n    rewards: torch.FloatTensor\n    ends: torch.LongTensor\n    mask_padding: torch.BoolTensor\n\n    def __post_init__(self):\n        assert len(self.observations) == len(self.actions) == len(self.rewards) == len(self.ends) == len(self.mask_padding)\n        if self.ends.sum() > 0:\n            idx_end = torch.argmax(self.ends) + 1\n            self.observations = self.observations[:idx_end]\n            self.actions = self.actions[:idx_end]\n            self.rewards = self.rewards[:idx_end]\n            self.ends = self.ends[:idx_end]\n            self.mask_padding = self.mask_padding[:idx_end]\n\n    def __len__(self) -> int:\n        return self.observations.size(0)\n\n    def merge(self, other: Episode) -> Episode:\n        return Episode(\n            torch.cat((self.observations, other.observations), dim=0),\n            torch.cat((self.actions, other.actions), dim=0),\n            torch.cat((self.rewards, other.rewards), dim=0),\n            torch.cat((self.ends, other.ends), dim=0),\n            torch.cat((self.mask_padding, other.mask_padding), dim=0),\n        )\n\n    def segment(self, start: int, stop: int, should_pad: bool = False) -> Episode:\n        assert start < len(self) and stop > 0 and start < stop\n        padding_length_right = max(0, stop - len(self))\n        padding_length_left = max(0, -start)\n        assert padding_length_right == padding_length_left == 0 or should_pad\n\n        def pad(x):\n            pad_right = torch.nn.functional.pad(x, [0 for _ in range(2 * x.ndim - 1)] + [padding_length_right]) if padding_length_right > 0 else x\n            return torch.nn.functional.pad(pad_right, [0 for _ in range(2 * x.ndim - 2)] + [padding_length_left, 0]) if padding_length_left > 0 else pad_right\n\n        start = max(0, start)\n        stop = min(len(self), stop)\n        segment = Episode(\n            self.observations[start:stop],\n            self.actions[start:stop],\n            self.rewards[start:stop],\n            self.ends[start:stop],\n            self.mask_padding[start:stop],\n        )\n\n        segment.observations = pad(segment.observations)\n        segment.actions = pad(segment.actions)\n        segment.rewards = pad(segment.rewards)\n        segment.ends = pad(segment.ends)\n        segment.mask_padding = torch.cat((torch.zeros(padding_length_left, dtype=torch.bool), segment.mask_padding, torch.zeros(padding_length_right, dtype=torch.bool)), dim=0)\n\n        return segment\n\n    def compute_metrics(self) -> EpisodeMetrics:\n        return EpisodeMetrics(len(self), self.rewards.sum())\n\n    def save(self, path: Path) -> None:\n        torch.save(self.__dict__, path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `Collector` is a class for collecting and managing experience from an environment, integrating agent actions, observations, rewards, and done signals, and updating a dataset with the collected episodes while handling various stages of the collection process.","metadata":{}},{"cell_type":"code","source":"class Collector:\n    def __init__(self, env: Union[SingleProcessEnv, MultiProcessEnv], dataset: EpisodesDataset, episode_dir_manager: EpisodeDirManager) -> None:\n        self.env = env\n        self.dataset = dataset\n        self.episode_dir_manager = episode_dir_manager\n        self.obs = self.env.reset()\n        self.episode_ids = [None] * self.env.num_envs\n        self.heuristic = RandomHeuristic(self.env.num_actions)\n\n    @torch.no_grad()\n    def collect(self, agent, epoch: int, epsilon: float, should_sample: bool, temperature: float, burn_in: int, *, num_steps: Optional[int] = None, num_episodes: Optional[int] = None):\n        print(f\"\\nEpoch {epoch} / {actual_epochs}\\n\")\n        assert self.env.num_actions == agent.world_model.act_vocab_size\n        assert 0 <= epsilon <= 1\n\n        assert (num_steps is None) != (num_episodes is None)\n        should_stop = lambda steps, episodes: steps >= num_steps if num_steps is not None else episodes >= num_episodes\n\n        to_log = []\n        steps, episodes = 0, 0\n        returns = []\n        observations, actions, rewards, dones = [], [], [], []\n\n        burnin_obs_rec, mask_padding = None, None\n        if set(self.episode_ids) != {None} and burn_in > 0:\n            current_episodes = [self.dataset[episode_id][0] for episode_id in self.episode_ids]\n            segmented_episodes = [episode.segment(start=len(episode) - burn_in, stop=len(episode), should_pad=True) for episode in current_episodes]\n            mask_padding = torch.stack([episode.mask_padding for episode in segmented_episodes], dim=0).to(\"cuda:0\")\n            burnin_obs = torch.stack([episode.observations for episode in segmented_episodes], dim=0).float().div(255).to(\"cuda:0\")\n            burnin_obs_rec = torch.clamp(agent.discrete_autoencoder.encode_decode(burnin_obs, should_preprocess=True, should_postprocess=True)[0], 0, 1)\n\n        agent.actor_critic.reset(n=self.env.num_envs, burnin_observations=burnin_obs_rec, mask_padding=mask_padding)\n        pbar = tqdm(total=num_steps if num_steps is not None else num_episodes, desc=f'Experience collection ({self.dataset.name})', file=sys.stdout)\n\n        while not should_stop(steps, episodes):\n\n            observations.append(self.obs)\n            obs = rearrange(torch.FloatTensor(self.obs).div(255), 'n h w c -> n c h w').to(\"cuda:0\")\n            act = agent.act(obs, should_sample=should_sample, temperature=temperature).cpu().numpy()\n\n            if random.random() < epsilon:\n                act = self.heuristic.act(obs).cpu().numpy()\n\n            self.obs, reward, done, _ = self.env.step(act)\n\n            actions.append(act)\n            rewards.append(reward)\n            dones.append(done)\n\n            new_steps = len(self.env.mask_new_dones)\n            steps += new_steps\n            pbar.update(new_steps if num_steps is not None else 0)\n\n            # Warning: with EpisodicLifeEnv + MultiProcessEnv, reset is ignored if not a real done.\n            # Thus, segments of experience following a life loss and preceding a general done are discarded.\n            # Not a problem with a SingleProcessEnv.\n\n            if self.env.should_reset():\n                self.add_experience_to_dataset(observations, actions, rewards, dones)\n\n                new_episodes = self.env.num_envs\n                episodes += new_episodes\n                pbar.update(new_episodes if num_episodes is not None else 0)\n\n                for episode_id in self.episode_ids:\n                    episode = self.dataset[episode_id][0]\n                    self.episode_dir_manager.save(episode, episode_id, epoch)\n                    metrics_episode = {k: v for k, v in episode.compute_metrics().__dict__.items()}\n                    metrics_episode['episode_num'] = episode_id\n                    metrics_episode['action_histogram'] = wandb.Histogram(np_histogram=np.histogram(episode.actions.numpy(), bins=np.arange(0, self.env.num_actions + 1) - 0.5, density=True))\n                    to_log.append({f'{self.dataset.name}/{k}': v for k, v in metrics_episode.items()})\n                    returns.append(metrics_episode['episode_return'])\n\n                self.obs = self.env.reset()\n                self.episode_ids = [None] * self.env.num_envs\n                agent.actor_critic.reset(n=self.env.num_envs)\n                observations, actions, rewards, dones = [], [], [], []\n\n        # Add incomplete episodes to dataset, and complete them later.\n        if len(observations) > 0:\n            self.add_experience_to_dataset(observations, actions, rewards, dones)\n\n        agent.actor_critic.clear()\n\n    def add_experience_to_dataset(self, observations: List[np.ndarray], actions: List[np.ndarray], rewards: List[np.ndarray], dones: List[np.ndarray]) -> None:\n        assert len(observations) == len(actions) == len(rewards) == len(dones)\n        for i, (o, a, r, d) in enumerate(zip(*map(lambda arr: np.swapaxes(arr, 0, 1), [observations, actions, rewards, dones]))):  # Make everything (N, T, ...) instead of (T, N, ...)\n            episode = Episode(\n                observations=torch.ByteTensor(o).permute(0, 3, 1, 2).contiguous(),  # channel-first\n                actions=torch.LongTensor(a),\n                rewards=torch.FloatTensor(r),\n                ends=torch.LongTensor(d),\n                mask_padding=torch.ones(d.shape[0], dtype=torch.bool),\n            )\n            if self.episode_ids[i] is None:\n                self.episode_ids[i] = self.dataset.add_episode(episode)\n            else:\n                self.dataset.update_episode(self.episode_ids[i], episode)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `EpisodesDataset` is a subclass of `torch.utils.data.Dataset` that manages a collection of episodes. It provides methods to add, update, and retrieve episodes, track modifications, and save dataset checkpoints. Additionally, it supports dynamic updates for sequence length and sampling methods.\n","metadata":{}},{"cell_type":"code","source":"Batch = Dict[str, torch.Tensor]\n\nclass EpisodesDataset(Dataset):\n    def __init__(self,  max_num_episodes: Optional[int] = None, name: Optional[str] = None,sequence_length: int = None, sample_from_start: bool = True, component: str = None) -> None:\n        self.max_num_episodes = max_num_episodes\n        self.name = name if name is not None else 'dataset'\n        self.num_seen_episodes = 0\n        self.episodes = deque()\n        self.episode_id_to_queue_idx = dict()\n        self.newly_modified_episodes, self.newly_deleted_episodes = set(), set()\n        self.sample_from_start = sample_from_start\n        self.sequence_length = sequence_length\n        self.component = component\n\n    def __len__(self) -> int:\n        return len(self.episodes)\n    \n    def __getitem__(self, episode_id: int) -> Episode:\n        assert episode_id in self.episode_id_to_queue_idx\n        queue_idx = self.episode_id_to_queue_idx[episode_id]\n        return (self.episodes[queue_idx],self.sample_from_start,self.sequence_length, self.component)\n    \n    def update_component(self,sequence_length, sample_from_start, component):\n        self.sequence_length = sequence_length\n        self.sample_from_start = sample_from_start\n        self.component = component\n        \n    def update_dataset(self,agent, epoch, epsilon,should_sample,temperature, num_steps, burn_in):\n        train_collector.collect(agent, epoch=epoch, epsilon=epsilon,should_sample= should_sample,temperature=temperature,num_steps= num_steps, burn_in=burn_in)\n\n    def update_episode(self, episode_id: int, new_episode: Episode) -> None:\n        assert episode_id in self.episode_id_to_queue_idx\n        queue_idx = self.episode_id_to_queue_idx[episode_id]\n        merged_episode = self.episodes[queue_idx].merge(new_episode)\n        self.episodes[queue_idx] = merged_episode\n        self.newly_modified_episodes.add(episode_id)\n\n    def _append_new_episode(self, episode):\n        episode_id = self.num_seen_episodes\n        self.episode_id_to_queue_idx[episode_id] = len(self.episodes)\n        self.episodes.append(episode)\n        self.num_seen_episodes += 1\n        self.newly_modified_episodes.add(episode_id)\n        return episode_id\n\n    def update_disk_checkpoint(self, directory: Path) -> None:\n        assert directory.is_dir()\n        for episode_id in self.newly_modified_episodes:\n            episode = self[episode_id][0]\n            episode.save(directory / f'{episode_id}.pt')\n        for episode_id in self.newly_deleted_episodes:\n            (directory / f'{episode_id}.pt').unlink()\n        self.newly_modified_episodes, self.newly_deleted_episodes = set(), set()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `EpisodesDatasetRamMonitoring` is a subclass of `EpisodesDataset` designed to manage episode storage within specified RAM usage limits. It dynamically adjusts the dataset size by removing older episodes if memory usage exceeds the set threshold, ensuring efficient RAM utilization.","metadata":{}},{"cell_type":"code","source":"class EpisodesDatasetRamMonitoring(EpisodesDataset):\n    \"\"\"\n    Prevent episode dataset from going out of RAM.\n    Warning: % looks at system wide RAM usage while G looks only at process RAM usage.\n    \"\"\"\n    def __init__(self,max_ram_usage: str, name: Optional[str] = None,sequence_length: int = None, sample_from_start: bool = True, component: str = None) -> None:\n        super().__init__(max_num_episodes=None, name=name, sequence_length= sequence_length,sample_from_start=sample_from_start,component=component)\n        self.max_ram_usage = max_ram_usage\n        self.num_steps = 0\n        self.max_num_steps = None\n\n        max_ram_usage = str(max_ram_usage)\n        if max_ram_usage.endswith('%'):\n            m = int(max_ram_usage.split('%')[0])\n            assert 0 < m < 100\n            self.check_ram_usage = lambda: psutil.virtual_memory().percent > m\n        else:\n            assert max_ram_usage.endswith('G')\n            m = float(max_ram_usage.split('G')[0])\n            self.check_ram_usage = lambda: psutil.Process().memory_info()[0] / 2 ** 30 > m\n\n    def clear(self) -> None:\n        super().clear()\n        self.num_steps = 0\n\n    def add_episode(self, episode: Episode) -> int:\n        if self.max_num_steps is None and self.check_ram_usage():\n            self.max_num_steps = self.num_steps\n        self.num_steps += len(episode)\n        while (self.max_num_steps is not None) and (self.num_steps > self.max_num_steps):\n            self._popleft()\n        episode_id = self._append_new_episode(episode)\n        return episode_id\n\n    def _popleft(self) -> Episode:\n        episode = super()._popleft()\n        self.num_steps -= len(episode)\n        return episode","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `EpisodeDirManager` manages the storage of episodes in a specified directory. It ensures that no more than a maximum number of episodes are saved by removing the oldest episodes when necessary. It also tracks and saves the episode with the best return, updating it if a new episode exceeds the previous best.\n","metadata":{}},{"cell_type":"code","source":"class EpisodeDirManager:\n    def __init__(self, episode_dir: Path, max_num_episodes: int) -> None:\n        self.episode_dir = episode_dir\n        self.episode_dir.mkdir(parents=False, exist_ok=True)\n        self.max_num_episodes = max_num_episodes\n        self.best_return = float('-inf')\n\n    def save(self, episode: Episode, episode_id: int, epoch: int) -> None:\n        if self.max_num_episodes is not None and self.max_num_episodes > 0:\n            self._save(episode, episode_id, epoch)\n\n    def _save(self, episode: Episode, episode_id: int, epoch: int) -> None:\n        ep_paths = [p for p in self.episode_dir.iterdir() if p.stem.startswith('episode_')]\n        assert len(ep_paths) <= self.max_num_episodes\n        if len(ep_paths) == self.max_num_episodes:\n            to_remove = min(ep_paths, key=lambda ep_path: int(ep_path.stem.split('_')[1]))\n            to_remove.unlink()\n        episode.save(self.episode_dir / f'episode_{episode_id}_epoch_{epoch}.pt')\n\n        ep_return = episode.compute_metrics().episode_return\n        if ep_return > self.best_return:\n            self.best_return = ep_return\n            path_best_ep = [p for p in self.episode_dir.iterdir() if p.stem.startswith('best_')]\n            assert len(path_best_ep) in (0, 1)\n            if len(path_best_ep) == 1:\n                path_best_ep[0].unlink()\n            episode.save(self.episode_dir / f'best_episode_{episode_id}_epoch_{epoch}.pt')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `RandomHeuristic` generates random actions for a given number of possible actions. It produces random actions uniformly across all available actions for each observation in a batch.","metadata":{}},{"cell_type":"code","source":"class RandomHeuristic:\n    def __init__(self, num_actions):\n        self.num_actions = num_actions\n\n    def act(self, obs):\n        assert obs.ndim == 4  # (N, H, W, C)\n        n = obs.size(0)\n        return torch.randint(low=0, high=self.num_actions, size=(n,))\n  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `IrisModel` and `IrisConfig` are imported from the `transformers` library. `IrisModel` is a model class for the Iris architecture, while `IrisConfig` provides configuration settings for initializing `IrisModel`.\n","metadata":{}},{"cell_type":"code","source":"from transformers import IrisModel, IrisConfig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `TrainableIRIS` is a subclass of `IrisModel` that extends the IRIS model to include a custom forward pass. It adds IRIS-specific loss calculations to the output and provides a method `original_forward` to access the base model's forward method without modifications.","metadata":{}},{"cell_type":"code","source":"class TrainableIRIS(IrisModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n    def forward(self, **kwargs):\n        output = super().forward(**kwargs)\n        # add the IRIS loss        \n        loss = output.losses.loss_total \n        return {\"loss\": loss}\n\n    def original_forward(self, **kwargs):\n        return super().forward(**kwargs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `iris_gym_data_collator` is a function that prepares batches of data for training by sampling episodes and segments based on the specified component type. It collates and processes data from episodes, adjusting the number of samples according to the component type and ensuring correct sequence lengths and batch formatting.","metadata":{}},{"cell_type":"code","source":"from functools import partial\nimport random\nfrom collections import deque\nbatch_num_samples_discrete_autoencoder = 128\nbatch_num_samples_world_model = 32\nbatch_num_samples_actor_critc = 32\n\ndef iris_gym_data_collator(samples):\n    sampled_episodes_segments = []\n    sample_from_start = samples[0][1]\n    sequence_length = samples[0][2]\n    episodes = [s[0] for s in samples]\n    episodes = deque(episodes)\n    \n    component = samples[0][3]\n    if component == \"discrete_autoencoder\":\n        batch_num_samples = batch_num_samples_discrete_autoencoder\n    elif component == \"world_model\":\n        batch_num_samples = batch_num_samples_world_model\n    else:\n        batch_num_samples = batch_num_samples_actor_critc\n        \n    sampled_episodes = random.choices(episodes, k=batch_num_samples)\n    for sampled_episode in sampled_episodes:\n        if sample_from_start:\n            start = random.randint(0, len(sampled_episode) - 1)\n            stop = start + sequence_length\n        else:\n            stop = random.randint(1, len(sampled_episode))\n            start = stop - sequence_length\n        sampled_episodes_segments.append(sampled_episode.segment(start, stop, should_pad=True))\n        assert len(sampled_episodes_segments[-1]) == sequence_length\n    sampled_episodes_segments = [e_s.__dict__ for e_s in sampled_episodes_segments]\n    batch = {}\n    for k in sampled_episodes_segments[0]:\n        batch[k] = torch.stack([e_s[k] for e_s in sampled_episodes_segments])\n    batch['observations'] = batch['observations'].float() / 255.0  # int8 to float and scale\n    return {\n            \"observations\": batch['observations'],\n            \"actions\": batch['actions'],\n            \"rewards\": batch['rewards'],\n            \"ends\": batch['ends'],\n            \"mask_padding\": batch['mask_padding'],\n            \"component\": component,\n            \"should_preprocess\": True,\n            \"should_postprocess\": True,\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `CustomTrainer` class for model training. Subclassing Hugging Faceâ€™s Trainer was necessary to develop a custom trainer class, which allowed for following the original training loop described in the paper. Only the _inner_training_loop function was overridden to meet the specific requirements.","metadata":{}},{"cell_type":"code","source":"import time\nfrom transformers.utils import logging,is_sagemaker_mp_enabled,is_torch_xla_available,is_accelerate_available,is_datasets_available\nfrom transformers.trainer_utils import has_length,seed_worker\nfrom transformers.debug_utils import DebugOption, DebugUnderflowOverflow\nfrom transformers.trainer_callback import TrainerState,ExportableState\nfrom transformers.trainer_pt_utils import get_model_param_count\nif is_accelerate_available():\n    from accelerate.utils import DistributedType\nif is_datasets_available():\n    import datasets\nlogger = logging.get_logger(__name__)\n\nworld_model_start_after_epochs = 5\nactor_critc_start_after_epochs = 10\n\nnum_env_steps = 200\n\nsequence_length_comp = [1,20,21]\nsample_from_start_comp = [True,True,False]\ncomponents_to_train = [\"discrete_autoencoder\", \"world_model\", \"actor_critic\"]\n\nclass CustomTrainer(Trainer):\n    def __init__(self, model,args = None,data_collator = None,train_dataset = None,eval_dataset = None,tokenizer = None,\n            model_init = None,compute_metrics = None,callbacks = None,optimizers = (None,None)):\n            \n            super().__init__(model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init,\n              compute_metrics, callbacks, optimizers)\n    \n    def _inner_training_loop(\n        self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None\n    ):\n        self.accelerator.free_memory()\n        self._train_batch_size = batch_size\n        if self.args.auto_find_batch_size:\n            if self.state.train_batch_size != self._train_batch_size:\n                from accelerate.utils import release_memory\n\n                (self.model_wrapped,) = release_memory(self.model_wrapped)\n                self.model_wrapped = self.model\n\n                # Check for DeepSpeed *after* the intial pass and modify the config\n                if self.is_deepspeed_enabled:\n                    # Temporarily unset `self.args.train_batch_size`\n                    original_bs = self.args.per_device_train_batch_size\n                    self.args.per_device_train_batch_size = self._train_batch_size // max(1, self.args.n_gpu)\n                    self.propagate_args_to_deepspeed(True)\n                    self.args.per_device_train_batch_size = original_bs\n            self.state.train_batch_size = self._train_batch_size\n        logger.debug(f\"Currently training with a batch size of: {self._train_batch_size}\")\n        # Data loader and number of training steps\n        train_dataloader = self.get_train_dataloader()\n        if self.is_fsdp_xla_v2_enabled:\n            train_dataloader = tpu_spmd_dataloader(train_dataloader)\n\n        # Setting up training control variables:\n        # number of training epochs: num_train_epochs\n        # number of training steps per epoch: num_update_steps_per_epoch\n        # total number of training steps to execute: max_steps\n        total_train_batch_size = self._train_batch_size * args.gradient_accumulation_steps * args.world_size\n\n        len_dataloader = None\n        num_train_tokens = None\n        if has_length(train_dataloader):\n            len_dataloader = len(train_dataloader)\n            num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps\n            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n            num_examples = self.num_examples(train_dataloader)\n            if args.max_steps > 0:\n                max_steps = args.max_steps\n                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n                    args.max_steps % num_update_steps_per_epoch > 0\n                )\n                # May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's\n                # the best we can do.\n                num_train_samples = args.max_steps * total_train_batch_size\n                if args.include_tokens_per_second:\n                    num_train_tokens = (\n                        self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\n                    )\n            else:\n                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n                num_train_epochs = math.ceil(args.num_train_epochs)\n                num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\n                if args.include_tokens_per_second:\n                    num_train_tokens = self.num_tokens(train_dataloader) * args.num_train_epochs\n        elif args.max_steps > 0:  # Rely on max_steps when dataloader does not have a working size\n            max_steps = args.max_steps\n            # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n            num_train_epochs = sys.maxsize\n            num_update_steps_per_epoch = max_steps\n            num_examples = total_train_batch_size * args.max_steps\n            num_train_samples = args.max_steps * total_train_batch_size\n            if args.include_tokens_per_second:\n                num_train_tokens = self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\n        else:\n            raise ValueError(\n                \"args.max_steps must be set to a positive value if dataloader does not have a length, was\"\n                f\" {args.max_steps}\"\n            )\n\n        if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n            if self.args.n_gpu > 1:\n                # nn.DataParallel(model) replicates the model, creating new variables and module\n                # references registered here no longer work on other gpus, breaking the module\n                raise ValueError(\n                    \"Currently --debug underflow_overflow is not supported under DP. Please use DDP\"\n                    \" (torchrun or torch.distributed.launch (deprecated)).\"\n                )\n            else:\n                debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n\n        delay_optimizer_creation = is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled or self.is_fsdp_enabled\n\n        # We need to reset the scheduler, as its parameters may be different on subsequent calls\n        if self._created_lr_scheduler:\n            self.lr_scheduler = None\n            self._created_lr_scheduler = False\n\n        if self.is_deepspeed_enabled:\n            self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\n\n        if not delay_optimizer_creation:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        self.state = TrainerState(\n            stateful_callbacks=[\n                cb for cb in self.callback_handler.callbacks + [self.control] if isinstance(cb, ExportableState)\n            ]\n        )\n        self.state.is_hyper_param_search = trial is not None\n        self.state.train_batch_size = self._train_batch_size\n\n        # Compute absolute values for logging, eval, and save if given as ratio\n        if args.logging_steps is not None:\n            if args.logging_steps < 1:\n                self.state.logging_steps = math.ceil(max_steps * args.logging_steps)\n            else:\n                self.state.logging_steps = args.logging_steps\n        if args.eval_steps is not None:\n            if args.eval_steps < 1:\n                self.state.eval_steps = math.ceil(max_steps * args.eval_steps)\n            else:\n                self.state.eval_steps = args.eval_steps\n        if args.save_steps is not None:\n            if args.save_steps < 1:\n                self.state.save_steps = math.ceil(max_steps * args.save_steps)\n            else:\n                self.state.save_steps = args.save_steps\n\n        # Activate gradient checkpointing if needed\n        if args.gradient_checkpointing:\n            if args.gradient_checkpointing_kwargs is None:\n                gradient_checkpointing_kwargs = {}\n            else:\n                gradient_checkpointing_kwargs = args.gradient_checkpointing_kwargs\n\n            self.model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n\n        model = self._wrap_model(self.model_wrapped)\n\n        # as the model is wrapped, don't use `accelerator.prepare`\n        # this is for unhandled cases such as\n        # FSDP-XLA, SageMaker MP/DP, DataParallel, IPEX\n        use_accelerator_prepare = True if model is self.model else False\n\n        if delay_optimizer_creation:\n            if use_accelerator_prepare:\n                self._fsdp_qlora_plugin_updates()\n                self.model = self.accelerator.prepare(self.model)\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        # prepare using `accelerator` prepare\n        if use_accelerator_prepare:\n            self.model.train()\n            if hasattr(self.lr_scheduler, \"step\"):\n                if self.use_apex:\n                    model = self.accelerator.prepare(self.model)\n                else:\n                    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n            else:\n                # to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\n                model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\n                    self.model, self.optimizer, self.lr_scheduler\n                )\n        elif self.args.optim in [OptimizerNames.LOMO, OptimizerNames.ADALOMO]:\n            # In this case we are in DDP + LOMO, which should be supported\n            self.optimizer = self.accelerator.prepare(self.optimizer)\n\n        if self.is_fsdp_enabled:\n            self.model = self.model_wrapped = model\n\n        # for the rest of this function `model` is the outside model, whether it was wrapped or not\n        if model is not self.model:\n            self.model_wrapped = model\n\n        # backward compatibility\n        if self.is_deepspeed_enabled:\n            self.deepspeed = self.model_wrapped\n\n        # ckpt loading\n        if resume_from_checkpoint is not None:\n            if self.is_deepspeed_enabled:\n                deepspeed_load_checkpoint(\n                    self.model_wrapped, resume_from_checkpoint, load_module_strict=not _is_peft_model(self.model)\n                )\n            elif is_sagemaker_mp_enabled() or self.is_fsdp_enabled:\n                self._load_from_checkpoint(resume_from_checkpoint, self.model_wrapped)\n\n        # Check if saved optimizer or scheduler states exist\n        self._load_optimizer_and_scheduler(resume_from_checkpoint)\n\n        # important: at this point:\n        # self.model         is the Transformers Model\n        # self.model_wrapped is DDP(Transformers Model), Deepspeed(Transformers Model),\n        # FSDP(Transformers Model), Dynamo Optimized Module(Transformers Model) etc.\n\n        # Train!\n        logger.info(\"***** Running training *****\")\n        logger.info(f\"  Num examples = {num_examples:,}\")\n        logger.info(f\"  Num Epochs = {num_train_epochs:,}\")\n        logger.info(f\"  Instantaneous batch size per device = {self.args.per_device_train_batch_size:,}\")\n        if self.args.per_device_train_batch_size != self._train_batch_size:\n            logger.info(f\"  Training with DataParallel so batch size has been adjusted to: {self._train_batch_size:,}\")\n        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}\")\n        logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n        logger.info(f\"  Total optimization steps = {max_steps:,}\")\n        logger.info(f\"  Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\")\n\n        self.state.epoch = 0\n        start_time = time.time()\n        epochs_trained = 0\n        steps_trained_in_current_epoch = 0\n        steps_trained_progress_bar = None\n\n        # Check if continuing training from a checkpoint\n        if resume_from_checkpoint is not None and os.path.isfile(\n            os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME)\n        ):\n            self.state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))\n            self.compare_trainer_and_checkpoint_args(self.args, self.state)\n            self._load_callback_state()\n            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n            if not args.ignore_data_skip:\n                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n                steps_trained_in_current_epoch *= args.gradient_accumulation_steps\n            else:\n                steps_trained_in_current_epoch = 0\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n            logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n            if not args.ignore_data_skip:\n                logger.info(\n                    f\"  Will skip the first {epochs_trained} epochs then the first\"\n                    f\" {steps_trained_in_current_epoch} batches in the first epoch.\"\n                )\n\n        # Update the references\n        self.callback_handler.model = self.model\n        self.callback_handler.optimizer = self.optimizer\n        self.callback_handler.lr_scheduler = self.lr_scheduler\n        self.callback_handler.train_dataloader = train_dataloader\n        if self.hp_name is not None and self._trial is not None:\n            # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial\n            # parameter to Train when using DDP.\n            self.state.trial_name = self.hp_name(self._trial)\n        if trial is not None:\n            assignments = trial.assignments if self.hp_search_backend == HPSearchBackend.SIGOPT else trial\n            self.state.trial_params = hp_params(assignments)\n        else:\n            self.state.trial_params = None\n        # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n        # to set this after the load.\n        self.state.max_steps = max_steps\n        self.state.num_train_epochs = num_train_epochs\n        self.state.is_local_process_zero = self.is_local_process_zero()\n        self.state.is_world_process_zero = self.is_world_process_zero()\n\n        # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n        tr_loss = torch.tensor(0.0).to(args.device)\n        # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n        self._total_loss_scalar = 0.0\n        self._globalstep_last_logged = self.state.global_step\n        model.zero_grad()\n        grad_norm: Optional[float] = None\n        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n\n        if args.eval_on_start:\n            self._evaluate(trial, ignore_keys_for_eval, skip_scheduler=True)\n\n        total_batched_samples = 0\n        \n        component_idx = 0\n        \n        for epoch in range(epochs_trained, num_train_epochs):\n            if epoch == 0:\n                print(f'Training {components_to_train[component_idx]} for {num_env_steps} steps:')   \n            \n            epoch_iterator = train_dataloader\n            if hasattr(epoch_iterator, \"set_epoch\"):\n                epoch_iterator.set_epoch(epoch)\n\n            # Reset the past mems state at the beginning of each epoch if necessary.\n            if args.past_index >= 0:\n                self._past = None\n\n            steps_in_epoch = (\n                len(epoch_iterator)\n                if len_dataloader is not None\n                else args.max_steps * args.gradient_accumulation_steps\n            )\n            self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)\n\n            if epoch == epochs_trained and resume_from_checkpoint is not None and steps_trained_in_current_epoch == 0:\n                self._load_rng_state(resume_from_checkpoint)\n\n            rng_to_sync = False\n            steps_skipped = 0\n            if steps_trained_in_current_epoch > 0:\n                epoch_iterator = skip_first_batches(epoch_iterator, steps_trained_in_current_epoch)\n                steps_skipped = steps_trained_in_current_epoch\n                steps_trained_in_current_epoch = 0\n                rng_to_sync = True\n\n            step = -1\n            \n            ##########################################component selection for training code##########################################\n            if epoch%num_env_steps == 0 and epoch!=0 and int(epoch/num_env_steps) > world_model_start_after_epochs:\n                component_idx += 1\n                if component_idx == 1:\n                    self.model.rl_agent.discrete_autoencoder.eval()\n                    epoch_iterator.dataset.update_component(sequence_length_comp[component_idx], sample_from_start_comp[component_idx], components_to_train[component_idx])\n                elif component_idx == 2 and int(epoch/num_env_steps) > actor_critc_start_after_epochs:\n                    self.model.rl_agent.world_model.eval()\n                    epoch_iterator.dataset.update_component(sequence_length_comp[component_idx], sample_from_start_comp[component_idx], components_to_train[component_idx])\n                else:\n                    component_idx = 0\n                    epoch_iterator.dataset.update_component(sequence_length_comp[component_idx], sample_from_start_comp[component_idx], components_to_train[component_idx])\n                    epoch_iterator.dataset.update_dataset(model.rl_agent, epoch=int((epoch/num_env_steps)+5), epsilon=0.01,should_sample= True,temperature=1.0,num_steps= 200, burn_in=20)\n                    self.model.train()\n            \n                print(f'Training {components_to_train[component_idx]} for {num_env_steps} steps:')\n            elif epoch%num_env_steps == 0 and  (int(epoch/num_env_steps) < world_model_start_after_epochs):\n                epoch_iterator.dataset.update_dataset(model.rl_agent, epoch=int((epoch/num_env_steps)+5), epsilon=0.01,should_sample= True,temperature=1.0,num_steps= 200, burn_in=20)\n                print(f'Training {components_to_train[component_idx]} for {num_env_steps} steps:')\n            ##########################################################################################################################\n            for step, inputs in enumerate(epoch_iterator):\n                total_batched_samples += 1\n\n                if self.args.include_num_input_tokens_seen:\n                    main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n                    if main_input_name not in inputs:\n                        logger.warning(\n                            \"Tried to track the number of tokens seen, however the current model is \"\n                            \"not configured properly to know what item is the input. To fix this, add \"\n                            \"a `main_input_name` attribute to the model class you are using.\"\n                        )\n                    else:\n                        input_device = inputs[main_input_name].device\n                        self.state.num_input_tokens_seen += torch.sum(\n                            self.accelerator.gather(\n                                torch.tensor(inputs[main_input_name].numel(), device=input_device, dtype=torch.int64)\n                            )\n                        ).item()\n                if rng_to_sync:\n                    self._load_rng_state(resume_from_checkpoint)\n                    rng_to_sync = False\n\n                # Skip past any already trained steps if resuming training\n                if steps_trained_in_current_epoch > 0:\n                    steps_trained_in_current_epoch -= 1\n                    if steps_trained_progress_bar is not None:\n                        steps_trained_progress_bar.update(1)\n                    if steps_trained_in_current_epoch == 0:\n                        self._load_rng_state(resume_from_checkpoint)\n                    continue\n                elif steps_trained_progress_bar is not None:\n                    steps_trained_progress_bar.close()\n                    steps_trained_progress_bar = None\n\n                if step % args.gradient_accumulation_steps == 0:\n                    self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n\n                with self.accelerator.accumulate(model):\n                    tr_loss_step = self.training_step(model, inputs)\n\n                if (\n                    args.logging_nan_inf_filter\n                    and not is_torch_xla_available()\n                    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n                ):\n                    # if loss is nan or inf simply add the average of previous logged losses\n                    tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n                else:\n                    if tr_loss.device != tr_loss_step.device:\n                        raise ValueError(\n                            f\"Calculated loss must be on the original device: {tr_loss.device} but device in use is {tr_loss_step.device}\"\n                        )\n                    tr_loss += tr_loss_step\n\n                self.current_flos += float(self.floating_point_ops(inputs))\n\n                is_last_step_and_steps_less_than_grad_acc = (\n                    steps_in_epoch <= args.gradient_accumulation_steps and (step + 1) == steps_in_epoch\n                )\n\n                if (\n                    total_batched_samples % args.gradient_accumulation_steps == 0\n                    or\n                    # last step in epoch but step is always smaller than gradient_accumulation_steps\n                    is_last_step_and_steps_less_than_grad_acc\n                ):\n                    # the `or` condition of `is_last_step_and_steps_less_than_grad_acc` is not covered\n                    # in accelerate. So, explicitly enable sync gradients to True in that case.\n                    if is_last_step_and_steps_less_than_grad_acc:\n                        self.accelerator.gradient_state._set_sync_gradients(True)\n\n                    # Gradient clipping\n                    if args.max_grad_norm is not None and args.max_grad_norm > 0:\n                        # deepspeed does its own clipping\n\n                        if is_sagemaker_mp_enabled() and args.fp16:\n                            _grad_norm = self.optimizer.clip_master_grads(args.max_grad_norm)\n                        elif self.use_apex:\n                            # Revert to normal clipping otherwise, handling Apex or full precision\n                            _grad_norm = nn.utils.clip_grad_norm_(\n                                amp.master_params(self.optimizer),\n                                args.max_grad_norm,\n                            )\n                        else:\n                            _grad_norm = self.accelerator.clip_grad_norm_(\n                                model.parameters(),\n                                args.max_grad_norm,\n                            )\n\n                        if (\n                            is_accelerate_available()\n                            and self.accelerator.distributed_type == DistributedType.DEEPSPEED\n                        ):\n                            grad_norm = model.get_global_grad_norm()\n                            # In some cases the grad norm may not return a float\n                            if hasattr(grad_norm, \"item\"):\n                                grad_norm = grad_norm.item()\n                        else:\n                            grad_norm = _grad_norm\n\n                    self.optimizer.step()\n\n                    self.control = self.callback_handler.on_optimizer_step(args, self.state, self.control)\n\n                    optimizer_was_run = not self.accelerator.optimizer_step_was_skipped\n                    if optimizer_was_run:\n                        # Delay optimizer scheduling until metrics are generated\n                        if not isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n                            self.lr_scheduler.step()\n\n                    model.zero_grad()\n                    self.state.global_step += 1\n                    self.state.epoch = epoch + (step + 1 + steps_skipped) / steps_in_epoch\n                    self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n\n                    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n                else:\n                    self.control = self.callback_handler.on_substep_end(args, self.state, self.control)\n\n                if self.control.should_epoch_stop or self.control.should_training_stop:\n                    # PyTorch/XLA relies on the data loader to insert the mark_step for\n                    # each step. Since we are breaking the loop early, we need to manually\n                    # insert the mark_step here.\n                    if is_torch_xla_available():\n                        xm.mark_step()\n                    break\n                 \n            if step < 0:\n                logger.warning(\n                    \"There seems to be not a single sample in your epoch_iterator, stopping training at step\"\n                    f\" {self.state.global_step}! This is expected if you're using an IterableDataset and set\"\n                    f\" num_steps ({max_steps}) higher than the number of available samples.\"\n                )\n                self.control.should_training_stop = True\n\n            self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n            self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n\n            if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n                if is_torch_xla_available():\n                    # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n                    xm.master_print(met.metrics_report())\n                else:\n                    logger.warning(\n                        \"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"\n                        \"configured. Check your training configuration if this is unexpected.\"\n                    )\n            if self.control.should_training_stop:\n                break\n\n        if args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of training\n            delattr(self, \"_past\")\n\n        logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n        if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n            # Wait for everyone to get here so we are sure the model has been saved by process 0.\n            if is_torch_xla_available():\n                xm.rendezvous(\"load_best_model_at_end\")\n            elif args.parallel_mode == ParallelMode.DISTRIBUTED:\n                dist.barrier()\n            elif is_sagemaker_mp_enabled():\n                smp.barrier()\n\n            self._load_best_model()\n\n        # add remaining tr_loss\n        self._total_loss_scalar += tr_loss.item()\n        effective_global_step = max(self.state.global_step, 0.001)  # Avoid ZeroDivisionError\n        train_loss = self._total_loss_scalar / effective_global_step\n\n        metrics = speed_metrics(\n            \"train\",\n            start_time,\n            num_samples=num_train_samples,\n            num_steps=self.state.max_steps,\n            num_tokens=num_train_tokens,\n        )\n        self.store_flos()\n        metrics[\"total_flos\"] = self.state.total_flos\n        metrics[\"train_loss\"] = train_loss\n\n        self.is_in_train = False\n\n        self._memory_tracker.stop_and_update_metrics(metrics)\n\n        self.log(metrics)\n\n        run_dir = self._get_output_dir(trial)\n        checkpoints_sorted = self._sorted_checkpoints(use_mtime=False, output_dir=run_dir)\n\n        # Delete the last checkpoint when save_total_limit=1 if it's different from the best checkpoint and process allowed to save.\n        if self.args.should_save and self.state.best_model_checkpoint is not None and self.args.save_total_limit == 1:\n            for checkpoint in checkpoints_sorted:\n                if not os.path.samefile(checkpoint, self.state.best_model_checkpoint):\n                    logger.info(f\"Deleting older checkpoint [{checkpoint}] due to args.save_total_limit\")\n                    shutil.rmtree(checkpoint)\n\n        self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n\n        # Wait for the checkpoint to be uploaded.\n        self._finish_current_push()\n\n        # After training we make sure to retrieve back the original forward pass method\n        # for the embedding layer by removing the forward post hook.\n        if self.neftune_noise_alpha is not None:\n            self._deactivate_neftune(self.model)\n\n        return TrainOutput(self.state.global_step, train_loss, metrics)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The code initializes a training environment for IRIS using `SingleProcessEnv` with the `BreakoutNoFrameskip-v4` Atari environment. Then we calculate the number of training epochs based on actual epochs and steps per epoch. Moving forward we set up directories for storing media and episodes, handling existing directories with warnings. The `EpisodeDirManager` and `EpisodesDatasetRamMonitoring` classes manage episode data and RAM usage, respectively, and the `Collector` class gathers data from the environment. \n\n### The model, `TrainableIRIS`, is configured and moved to the specified device. A pre-training phase updates the dataset with data collected by the model. Training arguments are defined for the Hugging Face `TrainingArguments`, specifying parameters like batch size, learning rate, and optimizer. A custom trainer class, `CustomTrainer`, is instantiated and used to train the model.\n","metadata":{}},{"cell_type":"code","source":"train_env = SingleProcessEnv(make_atari(\"BreakoutNoFrameskip-v4\",64,20000,30,4,True,False))\n\nactual_epochs = 600\nnum_steps = 200\nnum_train_epochs = int((actual_epochs*num_steps*len(components_to_train))/2.3)#2.3 is number of steps per epoch of trainer class(matching original code training)\n\nmedia_dir = Path('media')\nepisode_dir = media_dir / 'episodes'\ndevice = \"cuda:0\"\n\ntry:\n    media_dir.mkdir(exist_ok=False, parents=False)\nexcept FileExistsError:\n    warnings.warn(f\"Directory {media_dir} already exists, skipping creation.\")\n\ntry:\n    episode_dir.mkdir(exist_ok=False, parents=False)\nexcept FileExistsError:\n    warnings.warn(f\"Directory {episode_dir} already exists, skipping creation.\")\n    \nepisode_manager_train = EpisodeDirManager(episode_dir / 'train', max_num_episodes=10)\nglobal train_collector\ntrain_dataset = EpisodesDatasetRamMonitoring(\"30G\",\"train_dataset\",sequence_length_comp[0] ,sample_from_start_comp[0],components_to_train[0])\ntrain_collector = Collector(train_env, train_dataset, episode_manager_train)\n\nmodel = TrainableIRIS(IrisConfig())\nmodel.to(device)\nstart_after_collect_iterations = 5\n\nfor i in range (start_after_collect_iterations):\n    train_dataset.update_dataset(model.rl_agent, epoch=i, epsilon=0.01,should_sample= True,temperature=1.0,num_steps= 200, burn_in=20)\n    \ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/output/\",\n    remove_unused_columns=False,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=128,\n    learning_rate=1e-4,\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    optim=\"adamw_torch\",\n    max_grad_norm=10.0,\n)\n\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=iris_gym_data_collator,\n)\n\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://huggingface.co/ruffy369/iris-breakout/resolve/main/iris_trained_agent.gif\"\nalt=\"drawing\"/>\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}